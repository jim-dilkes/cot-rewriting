{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json, os\n",
    "\n",
    "pd.set_option(\"display.precision\", 3)\n",
    "\n",
    "# results_dir = \"./.archive/results/stage_1_rewrites\"\n",
    "# results_dir = \"./.archive/results/stage_4_redos\"\n",
    "results_dir = \"./.archive/results/stage_6_samples_final\"\n",
    "\n",
    "label_mapping_file = \"./results/experiment_label_mapping.tsv\"\n",
    "label_mapping = pd.read_csv(label_mapping_file, sep=\"\\t\")\n",
    "use_tasks = [\n",
    "        \"gsm8k\",\n",
    "        # \"tracking_shuffled_objects_three_objects\",\n",
    "        \"tracking_shuffled_objects_five_objects_multi\",\n",
    "        # \"coinflip_eight\",\n",
    "        \"prontoqa\",\n",
    "        \"logiqa-en\",\n",
    "        \"lsat-ar\",\n",
    "        \"navigate\",\n",
    "        \"aqua-rat\",\n",
    "        \"logical_deduction_five_objects_multi\"\n",
    "    ]\n",
    "\n",
    "task_name_mapping = {\n",
    "    \"gsm8k\": \"GSM8K\",\n",
    "    \"tracking_shuffled_objects/five_objects_multi\": \"Track5\",\n",
    "    \"coinflip_eight\": \"Coinflip\",\n",
    "    \"prontoqa\": \"ProntoQA\",\n",
    "    \"logiqa-en\": \"LogiQA\",\n",
    "    \"lsat-ar\": \"LSAT\",\n",
    "    \"navigate\": \"Nav\",\n",
    "    \"aqua-rat\": \"AQuA\",\n",
    "    \"logical_deduction/five_objects_multi\": \"Deduct5\"\n",
    "}\n",
    "\n",
    "use_dirs = [\n",
    "    # \"SolveValidateRewrite/gpt35_all_instruct_structured__stg6\",\n",
    "    # \"SolveValidateRewrite/gpt35_all_instruct_structured_T07__stg6\",\n",
    "    # \"SolveValidateRewrite/gpt35_all_pattern_structured__stg6\",\n",
    "    \"SolveValidateRewrite/gpt35_all_instruct__stg4\",\n",
    "    # \"SolveValidateRewrite/gpt35_all_instruct__stg4_T07\",\n",
    "    \"SolveValidateRewrite/gpt35_all_instruct_structured__stg6\",\n",
    "    # \"SolveValidateRewrite/gpt35_all_instruct_structured__stg6_T07\",\n",
    "    # \"SolveValidateRewrite/gpt35_all_pattern__stg4\"\n",
    "    # \"SolveValidateRewrite/gpt35_cot_instruct__rewrite_T07\",\n",
    "    # \"SolveValidateRewrite/gpt35_validate_framing__rewrite_T07\",\n",
    "    # \"SolveValidateRewrite/gpt35_validate_framing_rephrase_1__T07\",\n",
    "    # \"SolveValidateRewrite/gpt35_validate_framing_rephrase_2__T07\",\n",
    "    # \"SolveValidateRewrite/gpt35_validate_pattern__stg3\",\n",
    "    # \"SolveValidateRewrite/gpt35_validate_rewrite_pattern__stg3\",\n",
    "    # \"PromptWithAnswerExtraction/gpt35_cot_instruct_reframed__baseline\",\n",
    "    # \"PromptWithAnswerExtraction/gpt35_cot_instruct_reframed__baseline\"\n",
    "]\n",
    "\n",
    "filepath = os.path.join(results_dir, \"prontoqa\",\"SolveValidateRewrite/gpt35_cot_instruct__rewrite_T0\", \"results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metrics(json_examples):\n",
    "    dfs=[]\n",
    "    for ex in json_examples:\n",
    "        ex_dict = {}\n",
    "        ex_dict['n_responses'] = ex['response_count']\n",
    "        ex_dict['true_answer'] = ex['true_answer']\n",
    "        ex_dict['predicted_answer'] = ex['predicted_answer']\n",
    "        ex_dict['correct'] = ex['true_answer'] == ex['predicted_answer']\n",
    "        for i,res in enumerate(ex['response_pairs']):\n",
    "            ex_dict[f\"answer_{i}\"] = res['answer']\n",
    "        ex_dict['answer_0_correct'] = ex_dict['answer_0'] == ex_dict['true_answer']\n",
    "        dfs.append(pd.DataFrame(ex_dict,index=[ex['example_idx']]))\n",
    "    df = pd.concat(dfs)\n",
    "    # Create 'answer_1' column if it does not exist yet\n",
    "    if 'answer_1' not in df.columns:\n",
    "        df['answer_1'] = np.nan\n",
    "\n",
    "    total_examples = len(df)\n",
    "    df['is_rewrite'] = ~df['answer_1'].isna()\n",
    "    total_rewrites = df['is_rewrite'].sum()\n",
    "    \n",
    "    total_originally_correct = df['answer_0_correct'].sum()\n",
    "    total_originally_incorrect = (~df['answer_0_correct']).sum()\n",
    "    \n",
    "    total_incorrect_rewrites = (df['is_rewrite'] & ~df['answer_0_correct']).sum()\n",
    "    total_correct_rewrites = (df['is_rewrite'] & df['answer_0_correct']).sum()\n",
    "    \n",
    "\n",
    "    # Get rewrite correction accuracy\n",
    "    df_rewrites = df[~df['answer_1'].isna()]\n",
    "    df_rewrite_conversions = df_rewrites.groupby(['answer_0_correct','correct']).size()\n",
    "    correct_to_wrong_perc = df_rewrite_conversions[True][False] / df_rewrite_conversions[True].sum()\n",
    "    wrong_to_correct_perc = df_rewrite_conversions[False][True] / df_rewrite_conversions[False].sum()    \n",
    "\n",
    "    return {\n",
    "        'pre_rewrite_acc': df['answer_0_correct'].mean(),\n",
    "        'Rewrite/Total': total_rewrites/ total_examples, # Percent of all examples that were rewritten\n",
    "        'Rewrite Incorrect/All Rewrite': total_incorrect_rewrites / total_rewrites, # Percent of all rewrites that were of incorrect answers   \n",
    "        'Rewrite Correct/All Correct':  total_correct_rewrites / total_originally_correct, # Percent of all correct answers that were rewritten\n",
    "        'Rewrite Incorrect/All Incorrect': total_incorrect_rewrites / total_originally_incorrect, # Percent of all incorrect answers that were rewritten\n",
    "        'Correct To Incorrect': correct_to_wrong_perc, # Percent of initially correct rewrites than then become wrong \n",
    "        'Incorrect To Correct': wrong_to_correct_perc, # Percent of initially wrong rewrites than then become correct\n",
    "        'All correct': total_originally_correct,\n",
    "        'All incorrect' :   total_originally_incorrect,\n",
    "        'Correct Rewrites': total_correct_rewrites,\n",
    "        'Incorrect Rewrites': total_incorrect_rewrites,\n",
    "        'Total Rewrites': total_rewrites,\n",
    "        'Total Examples': total_examples\n",
    "      \n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for task in use_tasks:\n",
    "    for dir in use_dirs:\n",
    "        print(f\"Task: {task}, Run: {os.path.join(results_dir, task, dir, 'results.json')}\")\n",
    "        # try to open results.json from each directory, if it exists\n",
    "        try:\n",
    "            with open(os.path.join(results_dir, task, dir, \"results.json\"), \"r\") as f:\n",
    "                data_dict = json.load(f)\n",
    "                metrics_dict = {\n",
    "                    \"Task\": data_dict[\"Task\"],\n",
    "                    \"Run\": dir.split(\"/\")[-1],\n",
    "                    # \"N examples\": data_dict[\"Number of examples\"],\n",
    "                    # \"Number of correct\": data_dict[\"Number of correct\"],\n",
    "                    \"Accuracy\": data_dict[\"Accuracy\"],\n",
    "                }\n",
    "                metrics_dict |= extract_metrics(data_dict['Examples'])\n",
    "                dfs.append(pd.DataFrame(metrics_dict, index=[f\"{task}_{dir.split('/')[-1]}\"]))\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "    \n",
    "df = pd.concat(dfs)\n",
    "df = df.set_index([\"Run\",\"Task\"])\n",
    "df.sort_index(inplace=True) \n",
    "df.rename(index=task_name_mapping, columns={'pre_rewrite_acc':'Pre-Rewrite Accuracy'},inplace=True)\n",
    "# Rename index \"Run\" using label_mapping (join on label)\n",
    "df = df.copy().reset_index()\n",
    "df = df.merge(label_mapping[['label','Experiment']], left_on='Run', right_on='label', how='left')\n",
    "df.set_index(['Experiment','Task'], inplace=True)\n",
    "del df['label']\n",
    "del df['Run']\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.precision', 3)\n",
    "pd.set_option('display.float_format', '{:.1f}'.format)\n",
    "df*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_df = df[df.index.get_level_values(0).str.contains(\"Structured\")]\n",
    "# REmove Experiment index\n",
    "structured_df.index = structured_df.index.droplevel(0)\n",
    "structured_df = (structured_df*100).T\n",
    "print(structured_df.to_latex(float_format=\"{:0.1f}\".format))\n",
    "structured_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_df = df[df.index.get_level_values(0).str.contains(\"Instruction\")]\n",
    "# REmove Experiment index\n",
    "instruction_df.index = instruction_df.index.droplevel(0)\n",
    "instruction_df = (instruction_df*100).T\n",
    "print((instruction_df*100).T.to_latex(float_format=\"{:0.1f}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([structured_df, instruction_df], axis=0, keys=['Structured', 'Instruction'])\n",
    "# combined_df.swaplevel(0, 1, axis=0).sort_index(axis=0)\n",
    "custom_order = {label: idx for idx, label in enumerate(list(instruction_df.index))}\n",
    "combined_df = combined_df.swaplevel(0, 1, axis=0).sort_index(axis=0)\n",
    "\n",
    "print(combined_df.to_latex(float_format=\"{:0.1f}\".format))\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_means = (df.copy()*100).reset_index()\n",
    "del df_means['Task']\n",
    "df_means = df_means.groupby('Experiment').mean()\n",
    "# Transpose table\n",
    "df_means = df_means.T\n",
    "df_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print latex\n",
    "tbl=df_means.to_latex(float_format=\"{:0.1f}\".format, escape=False)\n",
    "print(tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
